{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================== [ setting ] ===================================================\n",
    "import gc\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "os.chdir(\"/Users/gimjiseong/Downloads/[ Project ]/[ Project ] Kaggle/T-Academy X Kakr\")\n",
    "\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "SEED = 2109\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================== [ Pre-processing ] ===============================================\n",
    "df_train = pd.read_csv('./data/train.csv')\n",
    "df_test  = pd.read_csv('./data/test.csv')\n",
    "\n",
    "df_train['income'] = np.where(df_train['income'] == '>50K', 1, 0)\n",
    "# df_train.drop(['education'], axis=1, inplace = True)\n",
    "# df_test.drop(['education'], axis=1, inplace = True)\n",
    "df_train[\"fnlwgt\"] = df_train[\"fnlwgt\"].apply(lambda x : np.log(x))\n",
    "df_test[\"fnlwgt\"] = df_test[\"fnlwgt\"].apply(lambda x : np.log(x))\n",
    "\n",
    "workclass_other = ['Without-pay', 'Never-worked', '?']\n",
    "df_train['workclass'] = df_train['workclass'].apply(lambda x: 'Other' if x in workclass_other else x)\n",
    "df_test['workclass'] = df_test['workclass'].apply(lambda x: 'Other' if x in workclass_other else x)\n",
    "\n",
    "def age_to_cat(x):\n",
    "    if x < 20:\n",
    "        return '10대 이하'\n",
    "    elif x < 30:\n",
    "        return '20대'\n",
    "    elif x < 40:\n",
    "        return '30대'\n",
    "    elif x < 50:\n",
    "        return '40대'\n",
    "    elif x < 60:\n",
    "        return '50대'\n",
    "    elif x < 70:\n",
    "        return '60대'\n",
    "    elif x < 80:\n",
    "        return '70대'\n",
    "    else:\n",
    "        return '80대 이상'\n",
    "    \n",
    "def age_to_level(x):\n",
    "    if x < 35:\n",
    "        return 'young_level'\n",
    "    elif x < 65:\n",
    "        return 'middle_level'\n",
    "    else:\n",
    "        return 'old_level'\n",
    "    \n",
    "df_train['age_to_cat'] = df_train['age'].apply(age_to_cat)\n",
    "df_test['age_to_cat'] = df_test['age'].apply(age_to_cat)\n",
    "df_train['age_to_level'] = df_train['age'].apply(age_to_level)\n",
    "df_test['age_to_level'] = df_test['age'].apply(age_to_level)\n",
    "\n",
    "def education_level(x):\n",
    "    if (x == 'Doctorate') or (x == 'Prof_school') or (x == 'Masters'):\n",
    "        return 'High_edu'\n",
    "    elif (x == 'Bachelors') or (x == 'Assoc_acdm') or (x == 'Assoc_voc') or (x == 'Some-college'):\n",
    "        return 'Middle-edu'\n",
    "    else:\n",
    "        return 'Low-edu'\n",
    "    \n",
    "df_train['education_level'] = df_train['education'].apply(education_level)\n",
    "df_test['education_level'] = df_test['education'].apply(education_level)\n",
    "\n",
    "df_train.loc[df_train['marital_status'] == 'Married-AF-spouse', 'marital_status'] = 'Married-civ-spouse'\n",
    "df_test.loc[df_test['marital_status'] == 'Married-AF-spouse', 'marital_status'] = 'Married-civ-spouse'\n",
    "\n",
    "def now_married(x):\n",
    "    if x == 'Married-civ-spouse':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "df_train['now_married'] = df_train['marital_status'].apply(now_married)\n",
    "df_test['now_married'] = df_test['marital_status'].apply(now_married)\n",
    "\n",
    "def public_officer(x):\n",
    "    if (x == 'State-gov') or (x == 'Loacl-gov') or (x == 'Federal-gov'):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "df_train['Public_worker'] = df_train['workclass'].apply(public_officer)\n",
    "df_test['Public_worker'] = df_test['workclass'].apply(public_officer)\n",
    "\n",
    "df_train.loc[df_train['occupation'].isin(['Armed-Forces', 'Priv-house-serv']), 'occupation'] = 'Priv-house-serv'\n",
    "df_test.loc[df_test['occupation'].isin(['Armed-Forces', 'Priv-house-serv']), 'occupation'] = 'Priv-house-serv'\n",
    "\n",
    "marital_status_data = df_train.groupby('marital_status').aggregate(np.mean)\n",
    "marital_status_ranks = {}\n",
    "rank = 1\n",
    "for idx, row in marital_status_data.sort_values(by='age').iterrows():\n",
    "    marital_status_ranks[idx] = rank\n",
    "    rank += 1\n",
    "for data in [df_train, df_test]:\n",
    "    marital_status_feature = []\n",
    "    for idx, row in data.iterrows():\n",
    "        marital_status_feature.append(marital_status_ranks[row.marital_status])\n",
    "    data['marital_status_ranks'] = marital_status_feature\n",
    "    \n",
    "occupation_data = df_train.groupby('occupation').aggregate(np.mean)\n",
    "occupation_ranks = {}\n",
    "rank = 1\n",
    "for idx, row in occupation_data.sort_values(by='age').iterrows():\n",
    "    occupation_ranks[idx] = rank\n",
    "    rank += 1\n",
    "for data in [df_train, df_test]:\n",
    "    occupation_feature = []\n",
    "    for idx, row in data.iterrows():\n",
    "        occupation_feature.append(occupation_ranks[row.occupation])\n",
    "    data['occupation_ranks'] = occupation_feature\n",
    "    \n",
    "workclass_data = df_train.groupby('workclass').aggregate(np.mean)\n",
    "workclass_ranks = {}\n",
    "rank = 1\n",
    "for idx, row in workclass_data.sort_values(by='age').iterrows():\n",
    "    workclass_ranks[idx] = rank\n",
    "    rank += 1\n",
    "for data in [df_train, df_test]:\n",
    "    workclass_feature = []\n",
    "    for idx, row in data.iterrows():\n",
    "        workclass_feature.append(workclass_ranks[row.workclass])\n",
    "    data['workclass_ranks'] = workclass_feature\n",
    "    \n",
    "df_train['capital_diff'] = df_train['capital_gain'] - df_train['capital_loss']\n",
    "df_test['capital_diff'] = df_test['capital_gain'] - df_test['capital_loss']\n",
    "\n",
    "df_train['hours_per_year'] = df_train['hours_per_week'] * 52\n",
    "df_train['hours_per_month'] = df_train['hours_per_week'] * 4\n",
    "df_train['hours_per_day'] = df_train['hours_per_week']/7\n",
    "\n",
    "df_test['hours_per_year'] = df_test['hours_per_week'] * 52\n",
    "df_test['hours_per_month'] = df_test['hours_per_week'] * 4\n",
    "df_test['hours_per_day'] = df_test['hours_per_week']/7\n",
    "\n",
    "df_train['Edu_CapGn_mean'] = df_train.groupby(['education'])['capital_gain'].transform('mean')\n",
    "df_test['Edu_CapGn_mean'] = df_test.groupby(['education'])['capital_gain'].transform('mean')\n",
    "# df_train['Edu_CapGn_std'] = df_train.groupby(['education'])['capital_gain'].transform('std')\n",
    "# df_test['Edu_CapGn_std'] = df_test.groupby(['education'])['capital_gain'].transform('std')\n",
    "\n",
    "df_train['Marr_CapGn_mean'] = df_train.groupby(['marital_status'])['capital_gain'].transform('mean')\n",
    "df_test['Marr_CapGn_mean'] = df_test.groupby(['marital_status'])['capital_gain'].transform('mean')\n",
    "# df_train['Marr_CapGn_std'] = df_train.groupby(['marital_status'])['capital_gain'].transform('std')\n",
    "# df_test['Marr_CapGn_std'] = df_test.groupby(['marital_status'])['capital_gain'].transform('std')\n",
    "\n",
    "df_train['Occ_CapGn_mean'] = df_train.groupby(['occupation'])['capital_gain'].transform('mean')\n",
    "df_test['Occ_CapGn_mean'] = df_test.groupby(['occupation'])['capital_gain'].transform('mean')\n",
    "# df_train['Occ_CapGn_std'] = df_train.groupby(['occupation'])['capital_gain'].transform('std')\n",
    "# df_test['Occ_CapGn_std'] = df_test.groupby(['occupation'])['capital_gain'].transform('std')\n",
    "\n",
    "df_train['Edu_Capdf_mean'] = df_train.groupby(['education'])['capital_diff'].transform('mean')\n",
    "df_test['Edu_Capdf_mean'] = df_test.groupby(['education'])['capital_diff'].transform('mean')\n",
    "# df_train['Edu_Capdf_std'] = df_train.groupby(['education'])['capital_diff'].transform('std')\n",
    "# df_test['Edu_Capdf_std'] = df_test.groupby(['education'])['capital_diff'].transform('std')\n",
    "\n",
    "df_train['Marr_Capdf_mean'] = df_train.groupby(['marital_status'])['capital_diff'].transform('mean')\n",
    "df_test['Marr_Capdf_mean'] = df_test.groupby(['marital_status'])['capital_diff'].transform('mean')\n",
    "# df_train['Marr_Capdf_std'] = df_train.groupby(['marital_status'])['capital_diff'].transform('std')\n",
    "# df_test['Marr_Capdf_std'] = df_test.groupby(['marital_status'])['capital_diff'].transform('std')\n",
    "\n",
    "df_train['Occ_Capdf_mean'] = df_train.groupby(['occupation'])['capital_diff'].transform('mean')\n",
    "df_test['Occ_Capdf_mean'] = df_test.groupby(['occupation'])['capital_diff'].transform('mean')\n",
    "# df_train['Occ_Capdf_std'] = df_train.groupby(['occupation'])['capital_diff'].transform('std')\n",
    "# df_test['Occ_Capdf_std'] = df_test.groupby(['occupation'])['capital_diff'].transform('std')\n",
    "\n",
    "# pos_key = df_train.loc[(df_train['income'] == 1) & (df_train['capital_diff'] > 0), 'capital_diff'].value_counts().sort_index().keys().tolist()\n",
    "# neg_key = df_train.loc[(df_train['income'] == 0) & (df_train['capital_diff'] > 0), 'capital_diff'].value_counts().sort_index().keys().tolist()\n",
    "\n",
    "# capital_diff_pos_key = [key for key in pos_key if key not in neg_key]\n",
    "# capital_diff_neg_key = [key for key in neg_key if key not in pos_key]\n",
    "# df_train['capital_diff_pos_key'] = df_train['capital_diff'].apply(lambda x: x in capital_diff_pos_key)\n",
    "# df_train['capital_diff_neg_key'] = df_train['capital_diff'].apply(lambda x: x in capital_diff_neg_key)\n",
    "# df_test['capital_diff_pos_key'] = df_test['capital_diff'].apply(lambda x: x in capital_diff_pos_key)\n",
    "# df_test['capital_diff_neg_key'] = df_test['capital_diff'].apply(lambda x: x in capital_diff_neg_key)\n",
    "\n",
    "# df_train.drop(['native_country'], axis=1, inplace = True)\n",
    "# df_test.drop(['native_country'], axis=1, inplace = True)\n",
    "\n",
    "y = df_train['income']\n",
    "df_train = df_train.drop('income', axis=1)\n",
    "\n",
    "for col in df_train.columns:\n",
    "    if df_train[col].dtype.name == 'object' or df_test[col].dtype.name == 'object':\n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(df_train[col].values) + list(df_test[col].values))\n",
    "        df_train[col] = le.transform(list(df_train[col].values))\n",
    "        df_test[col]  = le.transform(list(df_test[col].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  1.27 Mb (80.7% reduction)\n",
      "Mem. usage decreased to  0.32 Mb (80.7% reduction)\n"
     ]
    }
   ],
   "source": [
    "# ======================================== [ memory optimizaiton] ==============================================\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2   \n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        \n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "                    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    \n",
    "    if verbose: \n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "        \n",
    "    return df\n",
    "\n",
    "df_train = reduce_mem_usage(df_train)\n",
    "df_test  = reduce_mem_usage(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gimjiseong/opt/anaconda3/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/gimjiseong/opt/anaconda3/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/gimjiseong/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.297268\ttraining's f1: 0.866005\tvalid_1's binary_logloss: 0.318286\tvalid_1's f1: 0.851723\n",
      "[400]\ttraining's binary_logloss: 0.26175\ttraining's f1: 0.879357\tvalid_1's binary_logloss: 0.290934\tvalid_1's f1: 0.859855\n",
      "[600]\ttraining's binary_logloss: 0.246099\ttraining's f1: 0.885818\tvalid_1's binary_logloss: 0.285142\tvalid_1's f1: 0.860904\n",
      "Early stopping, best iteration is:\n",
      "[537]\ttraining's binary_logloss: 0.250149\ttraining's f1: 0.883831\tvalid_1's binary_logloss: 0.285967\tvalid_1's f1: 0.86225\n",
      "Fold 1 | F1 Score: 0.8622504093937868\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.297791\ttraining's f1: 0.865437\tvalid_1's binary_logloss: 0.306322\tvalid_1's f1: 0.851427\n",
      "[400]\ttraining's binary_logloss: 0.261968\ttraining's f1: 0.879464\tvalid_1's binary_logloss: 0.283975\tvalid_1's f1: 0.865115\n",
      "[600]\ttraining's binary_logloss: 0.246244\ttraining's f1: 0.885508\tvalid_1's binary_logloss: 0.279643\tvalid_1's f1: 0.871678\n",
      "[800]\ttraining's binary_logloss: 0.235585\ttraining's f1: 0.890694\tvalid_1's binary_logloss: 0.278754\tvalid_1's f1: 0.870372\n",
      "Early stopping, best iteration is:\n",
      "[723]\ttraining's binary_logloss: 0.239367\ttraining's f1: 0.888837\tvalid_1's binary_logloss: 0.278929\tvalid_1's f1: 0.872729\n",
      "Fold 2 | F1 Score: 0.8727286028066508\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.298737\ttraining's f1: 0.864503\tvalid_1's binary_logloss: 0.281971\tvalid_1's f1: 0.87971\n",
      "[400]\ttraining's binary_logloss: 0.2629\ttraining's f1: 0.878914\tvalid_1's binary_logloss: 0.253449\tvalid_1's f1: 0.887991\n",
      "[600]\ttraining's binary_logloss: 0.247214\ttraining's f1: 0.88492\tvalid_1's binary_logloss: 0.248269\tvalid_1's f1: 0.892042\n",
      "Early stopping, best iteration is:\n",
      "[480]\ttraining's binary_logloss: 0.255413\ttraining's f1: 0.881512\tvalid_1's binary_logloss: 0.250504\tvalid_1's f1: 0.892879\n",
      "Fold 3 | F1 Score: 0.892878730586925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.296478\ttraining's f1: 0.865651\tvalid_1's binary_logloss: 0.332137\tvalid_1's f1: 0.83239\n",
      "[400]\ttraining's binary_logloss: 0.260604\ttraining's f1: 0.880004\tvalid_1's binary_logloss: 0.311871\tvalid_1's f1: 0.851762\n",
      "[600]\ttraining's binary_logloss: 0.244939\ttraining's f1: 0.886719\tvalid_1's binary_logloss: 0.307878\tvalid_1's f1: 0.859489\n",
      "Early stopping, best iteration is:\n",
      "[579]\ttraining's binary_logloss: 0.246211\ttraining's f1: 0.886054\tvalid_1's binary_logloss: 0.308119\tvalid_1's f1: 0.859489\n",
      "Fold 4 | F1 Score: 0.8594888611043718\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.297546\ttraining's f1: 0.865439\tvalid_1's binary_logloss: 0.306235\tvalid_1's f1: 0.851207\n",
      "[400]\ttraining's binary_logloss: 0.261838\ttraining's f1: 0.879121\tvalid_1's binary_logloss: 0.283694\tvalid_1's f1: 0.850501\n",
      "Early stopping, best iteration is:\n",
      "[252]\ttraining's binary_logloss: 0.283565\ttraining's f1: 0.870687\tvalid_1's binary_logloss: 0.29603\tvalid_1's f1: 0.854039\n",
      "Fold 5 | F1 Score: 0.8540392318864386\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.29795\ttraining's f1: 0.864293\tvalid_1's binary_logloss: 0.298203\tvalid_1's f1: 0.868628\n",
      "[400]\ttraining's binary_logloss: 0.262032\ttraining's f1: 0.879025\tvalid_1's binary_logloss: 0.277086\tvalid_1's f1: 0.877574\n",
      "[600]\ttraining's binary_logloss: 0.246315\ttraining's f1: 0.885656\tvalid_1's binary_logloss: 0.275069\tvalid_1's f1: 0.878033\n",
      "Early stopping, best iteration is:\n",
      "[470]\ttraining's binary_logloss: 0.255265\ttraining's f1: 0.882262\tvalid_1's binary_logloss: 0.275433\tvalid_1's f1: 0.881441\n",
      "Fold 6 | F1 Score: 0.8814409979837586\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.298177\ttraining's f1: 0.86482\tvalid_1's binary_logloss: 0.299696\tvalid_1's f1: 0.873754\n",
      "[400]\ttraining's binary_logloss: 0.262526\ttraining's f1: 0.878549\tvalid_1's binary_logloss: 0.268506\tvalid_1's f1: 0.879043\n",
      "[600]\ttraining's binary_logloss: 0.246771\ttraining's f1: 0.88551\tvalid_1's binary_logloss: 0.260926\tvalid_1's f1: 0.878202\n",
      "Early stopping, best iteration is:\n",
      "[499]\ttraining's binary_logloss: 0.253471\ttraining's f1: 0.882507\tvalid_1's binary_logloss: 0.263398\tvalid_1's f1: 0.88167\n",
      "Fold 7 | F1 Score: 0.8816697845347413\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.297595\ttraining's f1: 0.865532\tvalid_1's binary_logloss: 0.305063\tvalid_1's f1: 0.862523\n",
      "Early stopping, best iteration is:\n",
      "[199]\ttraining's binary_logloss: 0.297934\ttraining's f1: 0.865331\tvalid_1's binary_logloss: 0.30532\tvalid_1's f1: 0.862523\n",
      "Fold 8 | F1 Score: 0.8625225337427718\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.297467\ttraining's f1: 0.864777\tvalid_1's binary_logloss: 0.314874\tvalid_1's f1: 0.848082\n",
      "[400]\ttraining's binary_logloss: 0.261731\ttraining's f1: 0.879529\tvalid_1's binary_logloss: 0.292859\tvalid_1's f1: 0.859207\n",
      "Early stopping, best iteration is:\n",
      "[342]\ttraining's binary_logloss: 0.268626\ttraining's f1: 0.876761\tvalid_1's binary_logloss: 0.296936\tvalid_1's f1: 0.861312\n",
      "Fold 9 | F1 Score: 0.8613123745071289\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.297563\ttraining's f1: 0.86435\tvalid_1's binary_logloss: 0.30691\tvalid_1's f1: 0.864663\n",
      "[400]\ttraining's binary_logloss: 0.262321\ttraining's f1: 0.879126\tvalid_1's binary_logloss: 0.279945\tvalid_1's f1: 0.874122\n",
      "Early stopping, best iteration is:\n",
      "[255]\ttraining's binary_logloss: 0.283089\ttraining's f1: 0.869866\tvalid_1's binary_logloss: 0.294582\tvalid_1's f1: 0.877204\n",
      "Fold 10 | F1 Score: 0.8772038626001756\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.297516\ttraining's f1: 0.865075\tvalid_1's binary_logloss: 0.312422\tvalid_1's f1: 0.85483\n",
      "[400]\ttraining's binary_logloss: 0.261857\ttraining's f1: 0.879078\tvalid_1's binary_logloss: 0.290885\tvalid_1's f1: 0.858057\n",
      "[600]\ttraining's binary_logloss: 0.245945\ttraining's f1: 0.885691\tvalid_1's binary_logloss: 0.285845\tvalid_1's f1: 0.859036\n",
      "Early stopping, best iteration is:\n",
      "[488]\ttraining's binary_logloss: 0.253567\ttraining's f1: 0.881977\tvalid_1's binary_logloss: 0.287711\tvalid_1's f1: 0.861109\n",
      "Fold 11 | F1 Score: 0.8611094410915558\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.297904\ttraining's f1: 0.864795\tvalid_1's binary_logloss: 0.301994\tvalid_1's f1: 0.866958\n",
      "[400]\ttraining's binary_logloss: 0.262247\ttraining's f1: 0.879014\tvalid_1's binary_logloss: 0.278132\tvalid_1's f1: 0.88297\n",
      "[600]\ttraining's binary_logloss: 0.246529\ttraining's f1: 0.885736\tvalid_1's binary_logloss: 0.272097\tvalid_1's f1: 0.879549\n",
      "Early stopping, best iteration is:\n",
      "[404]\ttraining's binary_logloss: 0.261763\ttraining's f1: 0.879623\tvalid_1's binary_logloss: 0.277904\tvalid_1's f1: 0.884272\n",
      "Fold 12 | F1 Score: 0.8842724272275828\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.29797\ttraining's f1: 0.864858\tvalid_1's binary_logloss: 0.302806\tvalid_1's f1: 0.865869\n",
      "[400]\ttraining's binary_logloss: 0.262266\ttraining's f1: 0.878555\tvalid_1's binary_logloss: 0.279445\tvalid_1's f1: 0.868265\n",
      "Early stopping, best iteration is:\n",
      "[207]\ttraining's binary_logloss: 0.295804\ttraining's f1: 0.865804\tvalid_1's binary_logloss: 0.301195\tvalid_1's f1: 0.868682\n",
      "Fold 13 | F1 Score: 0.8686819112979645\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.298586\ttraining's f1: 0.864912\tvalid_1's binary_logloss: 0.28777\tvalid_1's f1: 0.873501\n",
      "[400]\ttraining's binary_logloss: 0.263002\ttraining's f1: 0.878983\tvalid_1's binary_logloss: 0.260998\tvalid_1's f1: 0.881395\n",
      "[600]\ttraining's binary_logloss: 0.246967\ttraining's f1: 0.884943\tvalid_1's binary_logloss: 0.2549\tvalid_1's f1: 0.879473\n",
      "Early stopping, best iteration is:\n",
      "[518]\ttraining's binary_logloss: 0.252383\ttraining's f1: 0.882803\tvalid_1's binary_logloss: 0.255598\tvalid_1's f1: 0.881814\n",
      "Fold 14 | F1 Score: 0.8818137044772578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.298043\ttraining's f1: 0.864507\tvalid_1's binary_logloss: 0.302877\tvalid_1's f1: 0.863411\n",
      "[400]\ttraining's binary_logloss: 0.262533\ttraining's f1: 0.878812\tvalid_1's binary_logloss: 0.27689\tvalid_1's f1: 0.873344\n",
      "Early stopping, best iteration is:\n",
      "[325]\ttraining's binary_logloss: 0.271629\ttraining's f1: 0.874887\tvalid_1's binary_logloss: 0.282667\tvalid_1's f1: 0.875186\n",
      "Fold 15 | F1 Score: 0.8751863002527356\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.297483\ttraining's f1: 0.86525\tvalid_1's binary_logloss: 0.313576\tvalid_1's f1: 0.859798\n",
      "[400]\ttraining's binary_logloss: 0.261826\ttraining's f1: 0.879734\tvalid_1's binary_logloss: 0.288705\tvalid_1's f1: 0.866517\n",
      "[600]\ttraining's binary_logloss: 0.246051\ttraining's f1: 0.88552\tvalid_1's binary_logloss: 0.281402\tvalid_1's f1: 0.870548\n",
      "Early stopping, best iteration is:\n",
      "[419]\ttraining's binary_logloss: 0.259812\ttraining's f1: 0.880148\tvalid_1's binary_logloss: 0.287519\tvalid_1's f1: 0.872669\n",
      "Fold 16 | F1 Score: 0.8726692325359873\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.298318\ttraining's f1: 0.86294\tvalid_1's binary_logloss: 0.294647\tvalid_1's f1: 0.876619\n",
      "[400]\ttraining's binary_logloss: 0.262758\ttraining's f1: 0.879224\tvalid_1's binary_logloss: 0.263176\tvalid_1's f1: 0.888763\n",
      "[600]\ttraining's binary_logloss: 0.246895\ttraining's f1: 0.884754\tvalid_1's binary_logloss: 0.252959\tvalid_1's f1: 0.89261\n",
      "[800]\ttraining's binary_logloss: 0.236401\ttraining's f1: 0.889841\tvalid_1's binary_logloss: 0.250736\tvalid_1's f1: 0.889566\n",
      "Early stopping, best iteration is:\n",
      "[643]\ttraining's binary_logloss: 0.244449\ttraining's f1: 0.885901\tvalid_1's binary_logloss: 0.252229\tvalid_1's f1: 0.89497\n",
      "Fold 17 | F1 Score: 0.8949701147324333\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.298285\ttraining's f1: 0.864858\tvalid_1's binary_logloss: 0.292327\tvalid_1's f1: 0.873386\n",
      "[400]\ttraining's binary_logloss: 0.262987\ttraining's f1: 0.87879\tvalid_1's binary_logloss: 0.265717\tvalid_1's f1: 0.886617\n",
      "[600]\ttraining's binary_logloss: 0.24724\ttraining's f1: 0.884724\tvalid_1's binary_logloss: 0.256567\tvalid_1's f1: 0.88635\n",
      "[800]\ttraining's binary_logloss: 0.236755\ttraining's f1: 0.890158\tvalid_1's binary_logloss: 0.253833\tvalid_1's f1: 0.891135\n",
      "Early stopping, best iteration is:\n",
      "[779]\ttraining's binary_logloss: 0.237781\ttraining's f1: 0.889691\tvalid_1's binary_logloss: 0.254078\tvalid_1's f1: 0.892454\n",
      "Fold 18 | F1 Score: 0.892454466787085\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.297091\ttraining's f1: 0.865495\tvalid_1's binary_logloss: 0.326042\tvalid_1's f1: 0.847031\n",
      "[400]\ttraining's binary_logloss: 0.261713\ttraining's f1: 0.879049\tvalid_1's binary_logloss: 0.298584\tvalid_1's f1: 0.863757\n",
      "[600]\ttraining's binary_logloss: 0.245982\ttraining's f1: 0.885595\tvalid_1's binary_logloss: 0.291715\tvalid_1's f1: 0.861086\n",
      "Early stopping, best iteration is:\n",
      "[441]\ttraining's binary_logloss: 0.257678\ttraining's f1: 0.880905\tvalid_1's binary_logloss: 0.296224\tvalid_1's f1: 0.865336\n",
      "Fold 19 | F1 Score: 0.8653364618034908\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.297787\ttraining's f1: 0.864764\tvalid_1's binary_logloss: 0.309732\tvalid_1's f1: 0.854062\n",
      "[400]\ttraining's binary_logloss: 0.262318\ttraining's f1: 0.878876\tvalid_1's binary_logloss: 0.282339\tvalid_1's f1: 0.865919\n",
      "[600]\ttraining's binary_logloss: 0.246489\ttraining's f1: 0.885215\tvalid_1's binary_logloss: 0.273975\tvalid_1's f1: 0.869864\n",
      "Early stopping, best iteration is:\n",
      "[572]\ttraining's binary_logloss: 0.248191\ttraining's f1: 0.884565\tvalid_1's binary_logloss: 0.274414\tvalid_1's f1: 0.869864\n",
      "Fold 20 | F1 Score: 0.8698636992371406\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.297584\ttraining's f1: 0.865105\tvalid_1's binary_logloss: 0.307844\tvalid_1's f1: 0.850456\n",
      "[400]\ttraining's binary_logloss: 0.262336\ttraining's f1: 0.878979\tvalid_1's binary_logloss: 0.281093\tvalid_1's f1: 0.855463\n",
      "[600]\ttraining's binary_logloss: 0.246554\ttraining's f1: 0.886364\tvalid_1's binary_logloss: 0.270343\tvalid_1's f1: 0.853792\n",
      "Early stopping, best iteration is:\n",
      "[420]\ttraining's binary_logloss: 0.260267\ttraining's f1: 0.880022\tvalid_1's binary_logloss: 0.279537\tvalid_1's f1: 0.856864\n",
      "Fold 21 | F1 Score: 0.8568644581081425\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.297994\ttraining's f1: 0.865552\tvalid_1's binary_logloss: 0.297244\tvalid_1's f1: 0.857965\n",
      "[400]\ttraining's binary_logloss: 0.262559\ttraining's f1: 0.879142\tvalid_1's binary_logloss: 0.273577\tvalid_1's f1: 0.867351\n",
      "Early stopping, best iteration is:\n",
      "[322]\ttraining's binary_logloss: 0.271988\ttraining's f1: 0.875466\tvalid_1's binary_logloss: 0.27813\tvalid_1's f1: 0.868555\n",
      "Fold 22 | F1 Score: 0.8685554594064372\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.296627\ttraining's f1: 0.866215\tvalid_1's binary_logloss: 0.337707\tvalid_1's f1: 0.839929\n",
      "[400]\ttraining's binary_logloss: 0.261258\ttraining's f1: 0.88014\tvalid_1's binary_logloss: 0.315495\tvalid_1's f1: 0.85709\n",
      "[600]\ttraining's binary_logloss: 0.245352\ttraining's f1: 0.886483\tvalid_1's binary_logloss: 0.309641\tvalid_1's f1: 0.855316\n",
      "Early stopping, best iteration is:\n",
      "[425]\ttraining's binary_logloss: 0.25855\ttraining's f1: 0.881272\tvalid_1's binary_logloss: 0.314224\tvalid_1's f1: 0.857411\n",
      "Fold 23 | F1 Score: 0.8574108860269111\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.297429\ttraining's f1: 0.86573\tvalid_1's binary_logloss: 0.320947\tvalid_1's f1: 0.830705\n",
      "[400]\ttraining's binary_logloss: 0.261585\ttraining's f1: 0.879878\tvalid_1's binary_logloss: 0.297296\tvalid_1's f1: 0.848884\n",
      "[600]\ttraining's binary_logloss: 0.245606\ttraining's f1: 0.885798\tvalid_1's binary_logloss: 0.292203\tvalid_1's f1: 0.850117\n",
      "Early stopping, best iteration is:\n",
      "[508]\ttraining's binary_logloss: 0.251713\ttraining's f1: 0.88308\tvalid_1's binary_logloss: 0.293106\tvalid_1's f1: 0.853243\n",
      "Fold 24 | F1 Score: 0.8532428730868437\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.295874\ttraining's f1: 0.865936\tvalid_1's binary_logloss: 0.359036\tvalid_1's f1: 0.819555\n",
      "[400]\ttraining's binary_logloss: 0.260244\ttraining's f1: 0.880819\tvalid_1's binary_logloss: 0.336784\tvalid_1's f1: 0.825912\n",
      "Early stopping, best iteration is:\n",
      "[362]\ttraining's binary_logloss: 0.264516\ttraining's f1: 0.878919\tvalid_1's binary_logloss: 0.339014\tvalid_1's f1: 0.828347\n",
      "Fold 25 | F1 Score: 0.8283466629857811\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.296892\ttraining's f1: 0.865888\tvalid_1's binary_logloss: 0.331887\tvalid_1's f1: 0.854779\n",
      "[400]\ttraining's binary_logloss: 0.261373\ttraining's f1: 0.879656\tvalid_1's binary_logloss: 0.309811\tvalid_1's f1: 0.854931\n",
      "Early stopping, best iteration is:\n",
      "[327]\ttraining's binary_logloss: 0.270085\ttraining's f1: 0.876499\tvalid_1's binary_logloss: 0.31484\tvalid_1's f1: 0.860156\n",
      "Fold 26 | F1 Score: 0.8601555037638865\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.298099\ttraining's f1: 0.864465\tvalid_1's binary_logloss: 0.296685\tvalid_1's f1: 0.87104\n",
      "[400]\ttraining's binary_logloss: 0.262466\ttraining's f1: 0.878782\tvalid_1's binary_logloss: 0.269221\tvalid_1's f1: 0.877285\n",
      "Early stopping, best iteration is:\n",
      "[328]\ttraining's binary_logloss: 0.271059\ttraining's f1: 0.874684\tvalid_1's binary_logloss: 0.274121\tvalid_1's f1: 0.87898\n",
      "Fold 27 | F1 Score: 0.8789795321871343\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.298013\ttraining's f1: 0.864757\tvalid_1's binary_logloss: 0.301125\tvalid_1's f1: 0.861527\n",
      "[400]\ttraining's binary_logloss: 0.262278\ttraining's f1: 0.878842\tvalid_1's binary_logloss: 0.276803\tvalid_1's f1: 0.868422\n",
      "Early stopping, best iteration is:\n",
      "[341]\ttraining's binary_logloss: 0.269313\ttraining's f1: 0.876442\tvalid_1's binary_logloss: 0.280586\tvalid_1's f1: 0.872321\n",
      "Fold 28 | F1 Score: 0.872321241442857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.296974\ttraining's f1: 0.866181\tvalid_1's binary_logloss: 0.325498\tvalid_1's f1: 0.837805\n",
      "[400]\ttraining's binary_logloss: 0.261642\ttraining's f1: 0.880273\tvalid_1's binary_logloss: 0.295206\tvalid_1's f1: 0.842669\n",
      "[600]\ttraining's binary_logloss: 0.24596\ttraining's f1: 0.88548\tvalid_1's binary_logloss: 0.287658\tvalid_1's f1: 0.85308\n",
      "[800]\ttraining's binary_logloss: 0.235512\ttraining's f1: 0.891029\tvalid_1's binary_logloss: 0.286165\tvalid_1's f1: 0.855977\n",
      "[1000]\ttraining's binary_logloss: 0.226753\ttraining's f1: 0.895689\tvalid_1's binary_logloss: 0.285893\tvalid_1's f1: 0.858358\n",
      "Early stopping, best iteration is:\n",
      "[969]\ttraining's binary_logloss: 0.227999\ttraining's f1: 0.895525\tvalid_1's binary_logloss: 0.285932\tvalid_1's f1: 0.85967\n",
      "Fold 29 | F1 Score: 0.8596699866210793\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's binary_logloss: 0.297987\ttraining's f1: 0.865242\tvalid_1's binary_logloss: 0.298657\tvalid_1's f1: 0.860806\n",
      "[400]\ttraining's binary_logloss: 0.26235\ttraining's f1: 0.879636\tvalid_1's binary_logloss: 0.273307\tvalid_1's f1: 0.870578\n",
      "Early stopping, best iteration is:\n",
      "[350]\ttraining's binary_logloss: 0.268147\ttraining's f1: 0.87698\tvalid_1's binary_logloss: 0.276732\tvalid_1's f1: 0.871629\n",
      "Fold 30 | F1 Score: 0.8716291274601233\n",
      "\n",
      "Mean F1 score = 0.869302295989306\n",
      "OOF F1 score = 0.8694471437514184\n"
     ]
    }
   ],
   "source": [
    "# =========================================== [ ligthGBM model] ================================================\n",
    "X_train = df_train.drop(['id'], axis=1)\n",
    "X_test  = df_test.drop(['id'], axis=1) \n",
    "y_train = y\n",
    "\n",
    "def lgb_f1_score(y_hat, data):\n",
    "    y_true = data.get_label()\n",
    "    y_hat  = np.round(y_hat)\n",
    "    \n",
    "    return 'f1', f1_score(y_true, y_hat, average='weighted'), True\n",
    "\n",
    "ligtGBM_params = {\n",
    "          'objective': 'binary',\n",
    "          'max_depth': -1,\n",
    "          \"max_bin\" : 100,\n",
    "          'n_jobs': -1,\n",
    "          'learning_rate': 0.01,\n",
    "#           'scale_pos_weight' : 1.25, # 1.25 = 0.8725\n",
    "          'num_leaves': 50,\n",
    "          'min_data_in_leaf': 25,\n",
    "          'lambda_l1' : 0.21,\n",
    "          'lambda_l2': 1.371190, \n",
    "          'feature_fraction': 0.749778, \n",
    "          'bagging_fraction': 0.893392, \n",
    "          'boosting_type': 'gbdt',\n",
    "          'subsample_freq': 1,\n",
    "#           'subsample': 0.80,    \n",
    "          'colsample_bytree' : 0.8,\n",
    "          'early_stopping_round' : 200,\n",
    "          'n_estimators': 100000,\n",
    "          'verbose': -1,\n",
    "          'random_state': SEED,\n",
    "          }\n",
    "\n",
    "NFOLDS = 25\n",
    "folds = KFold(n_splits = NFOLDS)\n",
    "\n",
    "columns = X_train.columns\n",
    "splits = folds.split(X_train, y_train)\n",
    "y_preds = np.zeros(X_test.shape[0])\n",
    "y_oof = np.zeros(X_train.shape[0])\n",
    "score = 0\n",
    "\n",
    "feature_importances = pd.DataFrame()\n",
    "feature_importances['feature'] = columns\n",
    "\n",
    "oof_train = np.zeros((len(X_train), ))\n",
    "oof_test = np.zeros((len(X_test), ))\n",
    "\n",
    "for fold_n, (trn_idx, val_idx) in enumerate(splits):\n",
    "    X_trn, X_val = X_train[columns].iloc[trn_idx], X_train[columns].iloc[val_idx]\n",
    "    y_trn, y_val = y_train.iloc[trn_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    dtrain = lgb.Dataset(X_trn, label=y_trn)\n",
    "    dvalid = lgb.Dataset(X_val, label=y_val)\n",
    "    \n",
    "    clf = lgb.train(\n",
    "        ligtGBM_params,\n",
    "        dtrain,\n",
    "        valid_sets = [dtrain, dvalid],\n",
    "        verbose_eval = 200,\n",
    "        early_stopping_rounds = 200,\n",
    "        feval = lgb_f1_score\n",
    "    )\n",
    "    \n",
    "    feature_importances[f'fold_{fold_n+1}'] = clf.feature_importance()\n",
    "    \n",
    "    y_pred_val = clf.predict(X_val) \n",
    "    oof_train[val_idx] = y_pred_val\n",
    "    y_pred_val = [int(v >= 0.5) for v in y_pred_val]\n",
    "    \n",
    "    y_oof[val_idx] = y_pred_val\n",
    "    print(f\"Fold {fold_n + 1} | F1 Score: {f1_score(y_val, y_pred_val, average='weighted')}\")\n",
    "    \n",
    "    score += f1_score(y_val, y_pred_val, average='weighted') / NFOLDS\n",
    "    y_preds += clf.predict(X_test) / NFOLDS\n",
    "    oof_test += clf.predict(X_test) / NFOLDS\n",
    "          \n",
    "    del X_trn, X_val, y_trn, y_val\n",
    "    gc.collect()\n",
    "    \n",
    "print(f\"\\nMean F1 score = {score}\")\n",
    "print(f\"OOF F1 score = {f1_score(y, y_oof, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =========================================== [ CatBoost model] ================================================\n",
    "df_train[\"ligtgbm_oof\"] = oof_train\n",
    "df_test[\"ligtgbm_oof\"] = oof_test\n",
    "\n",
    "X_train = df_train\n",
    "X_test  = df_test\n",
    "y_train = y\n",
    "\n",
    "def lgb_f1_score(y_hat, data):\n",
    "    y_true = data.get_label()\n",
    "    y_hat  = np.round(y_hat)\n",
    "    \n",
    "    return 'f1', f1_score(y_true, y_hat, average='weighted'), True\n",
    "\n",
    "cat_params = {\n",
    "              'n_estimators' : 300000,\n",
    "              'learning_rate': 0.01,\n",
    "              'random_seed': SEED,\n",
    "              'metric_period' : 200,\n",
    "              'od_wait' : 455, # od_wait : 300 = 0.87109 , 350 = 0.87135, 400 = 0.87159, 450 = 0.87160\n",
    "              'depth': 50,\n",
    "              'border_count' : 254,\n",
    "              'l2_leaf_reg' : 30, # 'l2_leaf_reg' : 30 = 0.8177 \n",
    "              'min_data_in_leaf' : 16,\n",
    "              'bootstrap_type' : 'Bayesian',\n",
    "              'bagging_temperature' : 0.9,\n",
    "              'max_leaves' : 60, # max_leaves : 60 = 0.8719\n",
    "              'grow_policy' : 'Lossguide',\n",
    "#               'colsample_bylevel':0.7,\n",
    "              'eval_metric' : 'F1',\n",
    "              'task_type' : 'CPU'\n",
    "                } \n",
    "NFOLDS = 10\n",
    "folds = KFold(n_splits=NFOLDS)\n",
    "\n",
    "columns = X_train.columns\n",
    "splits = folds.split(X_train, y_train)\n",
    "y_preds_CAT = np.zeros(X_test.shape[0])\n",
    "y_oof = np.zeros(X_train.shape[0])\n",
    "score = 0\n",
    "\n",
    "oof_train_cat = np.zeros((len(X_train), ))\n",
    "oof_test_cat = np.zeros((len(X_test), ))\n",
    "\n",
    "for fold_n, (trn_idx, val_idx) in enumerate(splits):\n",
    "    X_trn, X_val = X_train[columns].iloc[trn_idx], X_train[columns].iloc[val_idx]\n",
    "    y_trn, y_val = y_train.iloc[trn_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    dtrain = lgb.Dataset(X_trn, label = y_trn)\n",
    "    dvalid = lgb.Dataset(X_val, label = y_val)\n",
    "    \n",
    "    estimator = CatBoostClassifier(**cat_params)        \n",
    "    clf = estimator.fit(\n",
    "        X_trn, y_trn,\n",
    "        eval_set = (X_val, y_val),\n",
    "        use_best_model = True)\n",
    "    \n",
    "    y_pred_val = clf.predict(X_val) \n",
    "    oof_train_cat[val_idx] = y_pred_val\n",
    "    y_pred_val = [int(v >= 0.5) for v in y_pred_val]\n",
    "    \n",
    "    y_oof[val_idx] = y_pred_val\n",
    "    print(f\"Fold {fold_n + 1} | F1 Score: {f1_score(y_val, y_pred_val, average='weighted')}\")\n",
    "    \n",
    "    score += f1_score(y_val, y_pred_val, average='weighted') / NFOLDS\n",
    "    y_preds_CAT += clf.predict(X_test) / NFOLDS\n",
    "    oof_test_cat += clf.predict(X_test) / NFOLDS\n",
    "          \n",
    "    del X_trn, X_val, y_trn, y_val\n",
    "    gc.collect()\n",
    "    \n",
    "print(f\"\\nMean F1 score = {score}\")\n",
    "print(f\"OOF F1 score = {f1_score(y, y_oof, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01295086, 0.52241161, 0.00996503, ..., 0.05543319, 0.22476706,\n",
       "       0.0164165 ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./data/sample_submission.csv')\n",
    "y_preds = np.where(y_preds >= 0.5, 1, 0)\n",
    "submission['prediction'] = y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1. , 1. , 1. , ..., 0.9, 1. , 1. ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightgmb_sub = pd.read_csv('./data/sample_submission.csv')\n",
    "cat_sub = pd.read_csv('./data/sample_submission.csv')\n",
    "submission = pd.read_csv('./data/sample_submission.csv')\n",
    "\n",
    "lightgmb_sub['prediction'] = y_preds\n",
    "cat_sub['prediction'] = y_preds_CAT\n",
    "submission['prediction'] = np.mean(lightgmb_sub['prediction'], cat_sub['prediction']) # lightgmb_sub['prediction'] * 0.6 + cat_sub['prediction'] * 0.4\n",
    "submission['prediction'] = np.where(submission['prediction'] >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  prediction\n",
       "0   0           0\n",
       "1   1           0\n",
       "2   2           0\n",
       "3   3           1\n",
       "4   4           1\n",
       "5   5           0\n",
       "6   6           0\n",
       "7   7           0\n",
       "8   8           0\n",
       "9   9           0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.to_csv('Kuda_lightGBM_CV25.csv', index=False)\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
